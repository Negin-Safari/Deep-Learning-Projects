{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp1n4zje7xV_",
        "outputId": "0cdee370-9d4b-4362-b209-947e3813902e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "XaLZUiSCQfYq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lKzB-tUt4IJN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Filtering + Testing**"
      ],
      "metadata": {
        "id": "OtjH1n-BQBdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "def lower_case(text):\n",
        "    lowered_case = \"\".join([i.lower() for i in text])\n",
        "    return lowered_case\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.append('')\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "def tokenization(text):\n",
        "    tokens = re.split('\\W+',text)\n",
        "    return tokens\n",
        "\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizer(text):\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text\n",
        "\n",
        "def remove_non_alphabets(text):\n",
        "    regex = re.compile('[^a-zA-Z]')\n",
        "    return regex.sub(' ', text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RGzVujS7-tl",
        "outputId": "1d58c725-9bf6-4e0e-8c24-d0cedbcd9c0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def FilterText(text):\n",
        "    return remove_stopwords(lemmatizer(tokenization(lower_case(remove_non_alphabets(remove_punctuation(text))))))"
      ],
      "metadata": {
        "id": "lkemBa7URR9Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**"
      ],
      "metadata": {
        "id": "_o_LZOrWQHiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('./drive/MyDrive/NN-HW4/twitter-suicidal_data.csv')\n",
        "x = train['tweet']\n",
        "y = train['intention']"
      ],
      "metadata": {
        "id": "1Z8aFHaJQMyZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [FilterText(text) for text in x]\n",
        "ys = [ele for ele in y]"
      ],
      "metadata": {
        "id": "sQQzS-c-UchM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "K85gkGqASnl_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FilterText(x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SVK9VcZRcy7",
        "outputId": "8914aabd-5fef-4899-a429-25c8b57836a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['life',\n",
              " 'meaningless',\n",
              " 'want',\n",
              " 'end',\n",
              " 'life',\n",
              " 'badly',\n",
              " 'life',\n",
              " 'completely',\n",
              " 'empty',\n",
              " 'dont',\n",
              " 'want',\n",
              " 'create',\n",
              " 'meaning',\n",
              " 'creating',\n",
              " 'meaning',\n",
              " 'pain',\n",
              " 'long',\n",
              " 'hold',\n",
              " 'back',\n",
              " 'urge',\n",
              " 'run',\n",
              " 'car',\n",
              " 'head',\n",
              " 'first',\n",
              " 'next',\n",
              " 'person',\n",
              " 'coming',\n",
              " 'opposite',\n",
              " 'way',\n",
              " 'stop',\n",
              " 'feeling',\n",
              " 'jealous',\n",
              " 'tragic',\n",
              " 'character',\n",
              " 'like',\n",
              " 'gomer',\n",
              " 'pile',\n",
              " 'swift',\n",
              " 'end',\n",
              " 'able',\n",
              " 'bring',\n",
              " 'life']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding Matrix: Word2Vec**"
      ],
      "metadata": {
        "id": "W3ggB4g1koJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CBOW model\n",
        "word2vec1 = gensim.models.Word2Vec(x_train, min_count = 1, vector_size = 300, window = 10)\n",
        "# Create Skip Gram model\n",
        "word2vec2 = gensim.models.Word2Vec(x_train, min_count = 1, vector_size = 300, window = 10, sg = 1)"
      ],
      "metadata": {
        "id": "g7dqwqR_ky3W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(\"Cosine similarity between 'meaningless' and 'empty' - CBOW : \", word2vec1.wv.similarity('meaningless', 'empty'))\n",
        "# Print results\n",
        "print(\"Cosine similarity between 'meaningless' and 'empty' - Skip Gram : \", word2vec2.wv.similarity('meaningless', 'empty'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJWdTA4AoA_j",
        "outputId": "06686894-95f6-4940-ca3a-79dda9d6519b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'meaningless' and 'empty' - CBOW :  0.98537517\n",
            "Cosine similarity between 'meaningless' and 'empty' - Skip Gram :  0.8748165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec1.save('./drive/MyDrive/NN-HW4/word2vec1.bin')\n",
        "word2vec2.save('./drive/MyDrive/NN-HW4/word2vec2.bin')"
      ],
      "metadata": {
        "id": "ldVs-nwMgRjz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = max([len(x) for x in x_train])\n",
        "word2vec1.wv.vectors = torch.tensor(word2vec1.wv.vectors).to(device)\n",
        "embedding_matrix = word2vec1.wv.vectors"
      ],
      "metadata": {
        "id": "ftoBcXRMkWon"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, data, labels, word2vec_model, max_sequence_length):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    self.word2vec_model = word2vec_model\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    indices = [self.word2vec_model.wv.key_to_index[token] for token in self.data[index] if token in self.word2vec_model.wv.key_to_index]\n",
        "    indices = indices[:self.max_sequence_length] + [0] * max(0, self.max_sequence_length - len(indices))\n",
        "    sample = {\"data\": torch.tensor(indices), \"label\": torch.tensor(self.labels[index], dtype=torch.float)}\n",
        "    return sample"
      ],
      "metadata": {
        "id": "KrK_qSlF1Rlp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, targets):\n",
        "  correct = ((predictions.reshape(targets.shape)>=0.5) == targets).sum().item()\n",
        "  total = targets.size(0)\n",
        "  accuracy = correct / total\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "jDUpn8ak4Kdo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM Neural Network**"
      ],
      "metadata": {
        "id": "K4m8F_WRmyv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "  def __init__(self, embedding_matrix, hidden_size, output_size):\n",
        "    super(CustomLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(embedding_matrix.size(0), embedding_matrix.size(1))\n",
        "    self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "    self.embedding.weight.requires_grad = False\n",
        "    self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    lstm_out, _ = self.lstm(embedded)\n",
        "    aggregated = torch.mean(lstm_out, dim=1)\n",
        "    # output = self.fc(lstm_out[-1, :, :])\n",
        "    output = self.fc(aggregated)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "wjoG78TulyES"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset = CustomDataset(x_train, y_train, word2vec1, max_sequence_length)\n",
        "dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "cw0aL4r0p1ar"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "output_size = 1\n",
        "lstm_model = CustomLSTM(embedding_matrix, hidden_size, output_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "fOe-Se6bqpEt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0.0\n",
        "  total_accuracy = 0.0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    data_batch = batch[\"data\"].to(device)\n",
        "    label_batch = batch[\"label\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = lstm_model(data_batch)\n",
        "    loss = criterion(outputs.squeeze(), label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_accuracy = calculate_accuracy(outputs, label_batch)\n",
        "    total_accuracy += batch_accuracy\n",
        "\n",
        "  average_loss = total_loss / len(dataloader)\n",
        "  average_acc = total_accuracy / len(dataloader)\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Accuracy: {average_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B98_ohk1rVVx",
        "outputId": "178652b2-a486-4d0a-aa20-46406a5a2738"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6201, Accuracy: 0.6435\n",
            "Epoch 2/20, Loss: 0.3562, Accuracy: 0.8606\n",
            "Epoch 3/20, Loss: 0.3129, Accuracy: 0.8757\n",
            "Epoch 4/20, Loss: 0.2962, Accuracy: 0.8792\n",
            "Epoch 5/20, Loss: 0.2845, Accuracy: 0.8853\n",
            "Epoch 6/20, Loss: 0.2751, Accuracy: 0.8898\n",
            "Epoch 7/20, Loss: 0.2704, Accuracy: 0.8917\n",
            "Epoch 8/20, Loss: 0.2614, Accuracy: 0.8954\n",
            "Epoch 9/20, Loss: 0.2571, Accuracy: 0.8949\n",
            "Epoch 10/20, Loss: 0.2564, Accuracy: 0.8975\n",
            "Epoch 11/20, Loss: 0.2553, Accuracy: 0.8956\n",
            "Epoch 12/20, Loss: 0.2473, Accuracy: 0.8988\n",
            "Epoch 13/20, Loss: 0.2489, Accuracy: 0.8961\n",
            "Epoch 14/20, Loss: 0.2390, Accuracy: 0.9014\n",
            "Epoch 15/20, Loss: 0.2457, Accuracy: 0.8994\n",
            "Epoch 16/20, Loss: 0.2352, Accuracy: 0.9042\n",
            "Epoch 17/20, Loss: 0.2401, Accuracy: 0.8999\n",
            "Epoch 18/20, Loss: 0.2347, Accuracy: 0.9057\n",
            "Epoch 19/20, Loss: 0.2334, Accuracy: 0.9039\n",
            "Epoch 20/20, Loss: 0.2387, Accuracy: 0.9009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset_test = CustomDataset(x_test, y_test, word2vec1, max_sequence_length)\n",
        "dataloader_test = DataLoader(custom_dataset_test, batch_size=64, shuffle=True)\n",
        "\n",
        "total_loss = 0.0\n",
        "total_accuracy = 0.0\n",
        "ops = []\n",
        "trgs = []\n",
        "\n",
        "for batch in dataloader_test:\n",
        "  data_batch = batch[\"data\"].to(device)\n",
        "  label_batch = batch[\"label\"].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = lstm_model(data_batch)\n",
        "    loss = criterion(outputs.squeeze(), label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    batch_accuracy = calculate_accuracy(outputs, label_batch)\n",
        "    total_accuracy += batch_accuracy\n",
        "\n",
        "    for el in outputs:\n",
        "      ops.append(Tensor.cpu(el >= 0.5).item())\n",
        "\n",
        "    for el in label_batch:\n",
        "      trgs.append(Tensor.cpu(el).item())\n",
        "\n",
        "average_loss = total_loss / len(dataloader_test)\n",
        "average_acc = total_accuracy / len(dataloader_test)\n",
        "\n",
        "print(f'Test, Loss: {average_loss:.4f}, Accuracy: {average_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_Akiekq6nbQ",
        "outputId": "1e028f0c-3396-4325-e126-e2c0b2a51a82"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test, Loss: 0.2728, Accuracy: 0.8885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(ops, trgs).ravel()\n",
        "tn, fp, fn, tp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP5drUgFuwbA",
        "outputId": "f3ad9f3c-b56f-43c0-cec5-a8e61eb854a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(939, 121, 80, 684)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(trgs, ops))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BijboL2S73Jp",
        "outputId": "6286a1cb-f456-43e6-eae0-013012b421b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.92      0.90      1019\n",
            "         1.0       0.90      0.85      0.87       805\n",
            "\n",
            "    accuracy                           0.89      1824\n",
            "   macro avg       0.89      0.89      0.89      1824\n",
            "weighted avg       0.89      0.89      0.89      1824\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-Layer LSTM Neural Network**"
      ],
      "metadata": {
        "id": "cVMlUNWpnH6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTwoLSTM(nn.Module):\n",
        "  def __init__(self, embedding_matrix, hidden_size, output_size):\n",
        "    super(CustomTwoLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(embedding_matrix.size(0), embedding_matrix.size(1))\n",
        "    self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "    self.embedding.weight.requires_grad = False\n",
        "    self.lstm = nn.LSTM(embedding_matrix.size(1), hidden_size, num_layers=2)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    lstm_out, _ = self.lstm(embedded)\n",
        "    aggregated = torch.mean(lstm_out, dim=1)\n",
        "    # output = self.fc(lstm_out[-1, :, :])\n",
        "    output = self.fc(aggregated)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "C1OadG3HSYuW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset = CustomDataset(x_train, y_train, word2vec1, max_sequence_length)\n",
        "dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "9K3xSGwmUmZ_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "output_size = 1\n",
        "twolstm_model = CustomTwoLSTM(embedding_matrix, hidden_size, output_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(twolstm_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "aZ-frgTZSvll"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0.0\n",
        "  total_accuracy = 0.0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    data_batch = batch[\"data\"].to(device)\n",
        "    label_batch = batch[\"label\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = twolstm_model(data_batch)\n",
        "    loss = criterion(outputs.squeeze(), label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_accuracy = calculate_accuracy(outputs, label_batch)\n",
        "    total_accuracy += batch_accuracy\n",
        "\n",
        "  average_loss = total_loss / len(dataloader)\n",
        "  average_acc = total_accuracy / len(dataloader)\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Accuracy: {average_acc:.4f}')"
      ],
      "metadata": {
        "id": "konyluH1SzH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17646ead-9837-4aec-8a43-bf1305dd13c3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6684, Accuracy: 0.5974\n",
            "Epoch 2/20, Loss: 0.5565, Accuracy: 0.7642\n",
            "Epoch 3/20, Loss: 0.4980, Accuracy: 0.7981\n",
            "Epoch 4/20, Loss: 0.4645, Accuracy: 0.8250\n",
            "Epoch 5/20, Loss: 0.4638, Accuracy: 0.8225\n",
            "Epoch 6/20, Loss: 0.4398, Accuracy: 0.8289\n",
            "Epoch 7/20, Loss: 0.4196, Accuracy: 0.8408\n",
            "Epoch 8/20, Loss: 0.4102, Accuracy: 0.8459\n",
            "Epoch 9/20, Loss: 0.4064, Accuracy: 0.8454\n",
            "Epoch 10/20, Loss: 0.3958, Accuracy: 0.8465\n",
            "Epoch 11/20, Loss: 0.3963, Accuracy: 0.8473\n",
            "Epoch 12/20, Loss: 0.3893, Accuracy: 0.8502\n",
            "Epoch 13/20, Loss: 0.3872, Accuracy: 0.8473\n",
            "Epoch 14/20, Loss: 0.3800, Accuracy: 0.8500\n",
            "Epoch 15/20, Loss: 0.3828, Accuracy: 0.8466\n",
            "Epoch 16/20, Loss: 0.3714, Accuracy: 0.8517\n",
            "Epoch 17/20, Loss: 0.3609, Accuracy: 0.8561\n",
            "Epoch 18/20, Loss: 0.3540, Accuracy: 0.8578\n",
            "Epoch 19/20, Loss: 0.3499, Accuracy: 0.8602\n",
            "Epoch 20/20, Loss: 0.3440, Accuracy: 0.8609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset_test = CustomDataset(x_test, y_test, word2vec1, max_sequence_length)\n",
        "dataloader_test = DataLoader(custom_dataset_test, batch_size=64, shuffle=True)\n",
        "\n",
        "total_loss = 0.0\n",
        "total_accuracy = 0.0\n",
        "ops = []\n",
        "trgs = []\n",
        "\n",
        "for batch in dataloader_test:\n",
        "  data_batch = batch[\"data\"].to(device)\n",
        "  label_batch = batch[\"label\"].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = twolstm_model(data_batch)\n",
        "    loss = criterion(outputs.squeeze(), label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    batch_accuracy = calculate_accuracy(outputs, label_batch)\n",
        "    total_accuracy += batch_accuracy\n",
        "\n",
        "    for el in outputs:\n",
        "      ops.append(Tensor.cpu(el >= 0.5).item())\n",
        "\n",
        "    for el in label_batch:\n",
        "      trgs.append(Tensor.cpu(el).item())\n",
        "\n",
        "average_loss = total_loss / len(dataloader_test)\n",
        "average_acc = total_accuracy / len(dataloader_test)\n",
        "\n",
        "print(f'Test, Loss: {average_loss:.4f}, Accuracy: {average_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JEUB5uo7f94",
        "outputId": "40f8a82e-141f-491c-81a2-6fc42f2b703a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test, Loss: 0.3360, Accuracy: 0.8696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(ops, trgs).ravel()\n",
        "tn, fp, fn, tp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1OGfgQqx-es",
        "outputId": "7b24ca3f-b31c-4fe8-c701-ad3db01cd3b1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(983, 199, 36, 606)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(trgs, ops))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuTkZVbI8B0Y",
        "outputId": "6a0f2b29-fe8e-4471-fbe8-754fd6daa80f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.96      0.89      1019\n",
            "         1.0       0.94      0.75      0.84       805\n",
            "\n",
            "    accuracy                           0.87      1824\n",
            "   macro avg       0.89      0.86      0.87      1824\n",
            "weighted avg       0.88      0.87      0.87      1824\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN + 2-Layer LSTM Neural Network**"
      ],
      "metadata": {
        "id": "ioSyB2SunKw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNNTwoLSTM(nn.Module):\n",
        "  def __init__(self, embedding_matrix, hidden_size, output_size):\n",
        "    super(CustomCNNTwoLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(embedding_matrix.size(0), embedding_matrix.size(1))\n",
        "    self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "    self.embedding.weight.requires_grad = False\n",
        "    self.conv = nn.Conv1d(embedding_matrix.size(1), 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.lstm = nn.LSTM(64, hidden_size, num_layers=2, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = self.conv(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "    batch_size, features, length = x.size()\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = x.reshape(batch_size, length, features)\n",
        "    lstm_out, _ = self.lstm(x)\n",
        "    aggregated = torch.mean(lstm_out, dim=1)\n",
        "    output = self.fc(aggregated)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "WcHRVT0lYScs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset = CustomDataset(x_train, y_train, word2vec1, max_sequence_length)\n",
        "dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "zx0FjDJ4Z7C8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "output_size = 1\n",
        "cnntwolstm_model = CustomCNNTwoLSTM(embedding_matrix, hidden_size, output_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(cnntwolstm_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "DfrQ_PppZ7wu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0.0\n",
        "  total_accuracy = 0.0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    data_batch = batch[\"data\"].to(device)\n",
        "    label_batch = batch[\"label\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = cnntwolstm_model(data_batch)\n",
        "    loss = criterion(outputs.squeeze(), label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_accuracy = calculate_accuracy(outputs, label_batch)\n",
        "    total_accuracy += batch_accuracy\n",
        "\n",
        "  average_loss = total_loss / len(dataloader)\n",
        "  average_acc = total_accuracy / len(dataloader)\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Accuracy: {average_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIAddxTFZ71f",
        "outputId": "f8b18b1c-ff48-404d-e5c3-4137e95f46c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6853, Accuracy: 0.5582\n",
            "Epoch 2/20, Loss: 0.6585, Accuracy: 0.6045\n",
            "Epoch 3/20, Loss: 0.4383, Accuracy: 0.8261\n",
            "Epoch 4/20, Loss: 0.3715, Accuracy: 0.8469\n",
            "Epoch 5/20, Loss: 0.3517, Accuracy: 0.8551\n",
            "Epoch 6/20, Loss: 0.3472, Accuracy: 0.8618\n",
            "Epoch 7/20, Loss: 0.3477, Accuracy: 0.8583\n",
            "Epoch 8/20, Loss: 0.3355, Accuracy: 0.8618\n",
            "Epoch 9/20, Loss: 0.3374, Accuracy: 0.8584\n",
            "Epoch 10/20, Loss: 0.3379, Accuracy: 0.8609\n",
            "Epoch 11/20, Loss: 0.3357, Accuracy: 0.8585\n",
            "Epoch 12/20, Loss: 0.3309, Accuracy: 0.8624\n",
            "Epoch 13/20, Loss: 0.3332, Accuracy: 0.8620\n",
            "Epoch 14/20, Loss: 0.3312, Accuracy: 0.8647\n",
            "Epoch 15/20, Loss: 0.3282, Accuracy: 0.8633\n",
            "Epoch 16/20, Loss: 0.3265, Accuracy: 0.8668\n",
            "Epoch 17/20, Loss: 0.3298, Accuracy: 0.8706\n",
            "Epoch 18/20, Loss: 0.3318, Accuracy: 0.8663\n",
            "Epoch 19/20, Loss: 0.3230, Accuracy: 0.8637\n",
            "Epoch 20/20, Loss: 0.3223, Accuracy: 0.8670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset_test = CustomDataset(x_test, y_test, word2vec1, max_sequence_length)\n",
        "dataloader_test = DataLoader(custom_dataset_test, batch_size=64, shuffle=True)\n",
        "\n",
        "total_loss = 0.0\n",
        "total_accuracy = 0.0\n",
        "ops = []\n",
        "trgs = []\n",
        "\n",
        "for batch in dataloader_test:\n",
        "  data_batch = batch[\"data\"].to(device)\n",
        "  label_batch = batch[\"label\"].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = cnntwolstm_model(data_batch)\n",
        "    loss = criterion(outputs.squeeze(), label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    batch_accuracy = calculate_accuracy(outputs, label_batch)\n",
        "    total_accuracy += batch_accuracy\n",
        "\n",
        "    for el in outputs:\n",
        "      ops.append(Tensor.cpu(el >= 0.5).item())\n",
        "\n",
        "    for el in label_batch:\n",
        "      trgs.append(Tensor.cpu(el).item())\n",
        "\n",
        "average_loss = total_loss / len(dataloader_test)\n",
        "average_acc = total_accuracy / len(dataloader_test)\n",
        "\n",
        "print(f'Test, Loss: {average_loss:.4f}, Accuracy: {average_acc:.4f}')"
      ],
      "metadata": {
        "id": "dcUlY5cV7j0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40594a0-afb2-4669-f0b9-a43189edb6b7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test, Loss: 0.3101, Accuracy: 0.8766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(ops, trgs).ravel()\n",
        "tn, fp, fn, tp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWPyk3GRyBv-",
        "outputId": "2169d489-e8bb-4cc3-f058-d421f99f249b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989, 195, 30, 610)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(trgs, ops))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdsXxaT58Djv",
        "outputId": "ea97bd4b-12c7-454c-f92e-90501a89d3b2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.97      0.90      1019\n",
            "         1.0       0.95      0.76      0.84       805\n",
            "\n",
            "    accuracy                           0.88      1824\n",
            "   macro avg       0.89      0.86      0.87      1824\n",
            "weighted avg       0.89      0.88      0.87      1824\n",
            "\n"
          ]
        }
      ]
    }
  ]
}